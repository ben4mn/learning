{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMkT_-yzu954"
      },
      "source": [
        "# Scripting to Cloud Based AI via API\n",
        "\n",
        "## Anthropic Test Case\n",
        "\n",
        "First we need to install our python application extension\n",
        "\n",
        "We will discuss each one as we go through.\n",
        "\n",
        "We will also be using a sample pipline with the following steps:\n",
        "\n",
        "\n",
        "1.   Pull data from existing system -> SF Chat Transcripts\n",
        "2.   Format Data and upload to Google Colab\n",
        "3. Upload the excel file into a Panda Data Scheme using Python\n",
        "4. Scrub data through several passes to remove uneeded html characters and PII\n",
        "5. Use the data as a prompt when interacting with AI (Summarize and coach the convo)\n",
        "6. Analyze the AI Output JSON and format it into the PD\n",
        "7. Export the PD as a single file\n",
        "\n",
        "This shows what can be done as a POC to help define and design a fully automated AI pipline where these steps and triggers happen automatically\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbPuBKWtvjpX"
      },
      "outputs": [],
      "source": [
        "!pip install anthropic\n",
        "!pip install pandas\n",
        "!pip install google-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJsYp6GcvmHf"
      },
      "source": [
        "Next we can start scripting to against the response engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX4kZZomvp9n"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=\"APIKEY\"\n",
        ")\n",
        "message = client.messages.create(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    system=\"You are a world-class poet. Respond only with short poems.\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Why is the ocean salty?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "if message.content:\n",
        "      print(\"\\n\" + \"=\" * 50)\n",
        "      print(\"Claude's Response:\")\n",
        "      print(\"=\" * 50)\n",
        "      print(message.content[0].text)\n",
        "      print(\"=\" * 50 + \"\\n\")\n",
        "else:\n",
        "      print(\"No response generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hjZpKWNyA7m"
      },
      "source": [
        "Changing the prompt and parent prompt as an example - note the token cost on Anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_e_nmalwrg3"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=\"APIKEY\"\n",
        ")\n",
        "message = client.messages.create(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    system=\"You are a helpful assistant\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What are some of the tenants to good project management?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "if message.content:\n",
        "      print(\"\\n\" + \"=\" * 50)\n",
        "      print(\"Claude's Response:\")\n",
        "      print(\"=\" * 50)\n",
        "      print(message.content[0].text)\n",
        "      print(\"=\" * 50 + \"\\n\")\n",
        "else:\n",
        "      print(\"No response generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVgTZNHt1ayt"
      },
      "source": [
        "## Scripting Bulk Requests\n",
        "\n",
        "Now that we have successfully created a connection to Claude - we can script a bulk request by uploading a sample data set, teaching  the AI how to access the data for each prompt, and then looping through the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTVX3DQSyRBX"
      },
      "source": [
        "### Step 1\n",
        "\n",
        "In this example lets walk through uploading our exel file of data. If needed we can walk through how to export this data from Salesforce or other tools (for Salesforce I like to use workbench) but for now we will assume we already have data to use.\n",
        "\n",
        "We are going to use whats called a panda data structure, these are popular dimmensial type arrays that are frequently used in python for large data manipulation and data science. You can read more about them [here](https://pandas.pydata.org/docs/index.html) (or ask AI).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_l0C14pyj9X"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # pandas is used for data manipulation and analysis\n",
        "from google.colab import files  # This module helps with file uploads in Google Colab\n",
        "import io  # io provides Python's main facilities for dealing with various types of I/O\n",
        "\n",
        "# Define a function to upload and load an Excel file\n",
        "def upload_and_load_excel():\n",
        "    try:\n",
        "        # Prompt the user to upload a file\n",
        "        print(\"Please upload your Excel file.\")\n",
        "\n",
        "        # Use Google Colab's file upload feature\n",
        "        # This will open a file picker for the user to select a file\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        # Check if a file was actually uploaded\n",
        "        if not uploaded:\n",
        "            print(\"No file was uploaded.\")\n",
        "            return None  # Return None if no file was uploaded\n",
        "\n",
        "        # Get the name of the uploaded file\n",
        "        # 'next(iter(uploaded))' gets the first (and only) key from the uploaded dictionary\n",
        "        file_name = next(iter(uploaded))\n",
        "        print(f\"File '{file_name}' has been uploaded successfully.\")\n",
        "\n",
        "        # Read the uploaded Excel file into a pandas DataFrame\n",
        "        # BytesIO creates a file-like object in memory, which pandas can read\n",
        "        df = pd.read_excel(io.BytesIO(uploaded[file_name]), engine='openpyxl')\n",
        "\n",
        "        # Check if the DataFrame has at least two columns\n",
        "        # df.shape returns a tuple with (number of rows, number of columns)\n",
        "        if df.shape[1] < 2:\n",
        "            # Raise an error if there aren't enough columns\n",
        "            raise ValueError(\"The Excel file must contain at least two columns.\")\n",
        "\n",
        "        # Print information about the loaded data\n",
        "        print(f\"Successfully loaded data with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "        # Return the DataFrame for further use\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any error occurs during the process, print the error message\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None  # Return None to indicate that the operation failed\n",
        "\n",
        "# Call the function to upload and load the Excel file\n",
        "print(\"Starting the Excel file upload and load process...\")\n",
        "df = upload_and_load_excel()\n",
        "\n",
        "# Check if the DataFrame was successfully created\n",
        "if df is not None:\n",
        "    # If df is not None, it means the file was successfully loaded\n",
        "    print(\"\\nFirst few rows of the data:\")\n",
        "    print(df.head())  # Display the first 5 rows of the DataFrame\n",
        "\n",
        "    print(\"\\nDataFrame info:\")\n",
        "    df.info()  # Print a concise summary of the DataFrame\n",
        "else:\n",
        "    # If df is None, it means there was an error in loading the file\n",
        "    print(\"Failed to load the data. Please check the error message above and try again.\")\n",
        "\n",
        "# Note: After running this code, 'df' will contain your Excel data as a pandas DataFrame,\n",
        "# which you can use for further analysis or processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqxyeWQB03_2"
      },
      "source": [
        "We can test that our data is in a Panda DF by printing it (note that code you have run previously in the notebook and variables that are output are available to you as you work)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NU1r7uC0_Ym"
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5bkyM0c1CMO"
      },
      "source": [
        "### Step 2\n",
        "\n",
        "Now that we have our data in our system memory in the form of a Data Frame, we can begin to manipulate it, since we are using a public AI and there is a per token cost, lets see if we can reduce the number of tokens. Also since this is company information and I am not in a closed environment I need to ensure I have removed all PII."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAjb5RP61kjW"
      },
      "source": [
        "Lets import some public libraries that clean out CC, email and other payment information. These libraries are publicaly available for exercises like this. You can read more about them here. [Sanityze](https://pypi.org/project/sanitize/#description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9fYt1TO1Wl2"
      },
      "outputs": [],
      "source": [
        "!pip install bleach\n",
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNdtLaHP2EMt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import bleach\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "\n",
        "    # Convert to string if it's not already\n",
        "    text = str(text)\n",
        "\n",
        "    # Use bleach to clean HTML tags\n",
        "    cleaned = bleach.clean(text, tags=[], strip=True)\n",
        "\n",
        "    # Remove extra whitespace and newlines\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "    # Remove any remaining HTML entities\n",
        "    cleaned = bleach.clean(cleaned, strip=True)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def clean_dataframe(df, columns=None):\n",
        "    # If no columns specified, clean all object (string) columns\n",
        "    if columns is None:\n",
        "        columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Apply the clean_text function to specified columns\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(clean_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load your DataFrame (assuming you've already done this)\n",
        "# df = pd.read_excel('your_file.xlsx')\n",
        "\n",
        "# Clean the DataFrame\n",
        "bleacheddf = clean_dataframe(df)\n",
        "\n",
        "# If you want to clean only specific columns:\n",
        "# newdf = clean_dataframe(df, columns=['column1', 'column2'])\n",
        "\n",
        "print(bleacheddf.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvn6Vqbg3Ldv"
      },
      "source": [
        "At any point we can convert our Panda data frame back to an excel or csv, in this case I am going to export the bleachedDF so we can compare to our original.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2ZceqvNG57ne",
        "outputId": "c07497b4-3b0c-4933-c76d-d41998ed8c8e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6111731d-47ee-40be-9b76-096c54de729e\", \"example_export.xlsx\", 14434)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from google.colab import files\n",
        "\n",
        "def export_df_to_excel(bleacheddf, filename='dataframe_export.xlsx'):\n",
        "    \"\"\"\n",
        "    Export a pandas DataFrame to an Excel file and download it in Google Colab.\n",
        "\n",
        "    Parameters:\n",
        "    bleacheddf (pandas.DataFrame): The DataFrame to export\n",
        "    filename (str): The name of the file to be downloaded (default is 'dataframe_export.xlsx')\n",
        "    \"\"\"\n",
        "    # Create a BytesIO object\n",
        "    output = BytesIO()\n",
        "\n",
        "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
        "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
        "        # Write the DataFrame to the Excel file\n",
        "        bleacheddf.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "\n",
        "        # Get the xlsxwriter workbook and worksheet objects\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "        # Set the column widths (adjust as needed)\n",
        "        for i, col in enumerate(bleacheddf.columns):\n",
        "            column_len = max(bleacheddf[col].astype(str).str.len().max(), len(col) + 2)\n",
        "            worksheet.set_column(i, i, column_len)\n",
        "\n",
        "    # Reset the buffer position to the beginning\n",
        "    output.seek(0)\n",
        "\n",
        "    # Write the BytesIO content to the file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(output.getvalue())\n",
        "\n",
        "    # Use google.colab.files.download to trigger the file download\n",
        "    files.download(filename)\n",
        "\n",
        "# Example usage\n",
        "# Create a sample DataFrame\n",
        "data = bleacheddf\n",
        "bleacheddf = pd.DataFrame(data)\n",
        "\n",
        "# Export the DataFrame to an Excel file\n",
        "export_df_to_excel(bleacheddf, 'example_export.xlsx')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR8--Gtb6in1"
      },
      "source": [
        "As you can see we have already made the input much clearer and cleaner and reduced the size of our input text (or character count). Sometimes it can be helpful to include extra data in your prompt but not in this case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXWQbxB16qBN"
      },
      "source": [
        "### Step 3\n",
        "\n",
        "At this point we can remove names for extra security (this would be another step in the pipline or another pass in the pipeline mentioned in the intro)\n",
        "\n",
        "We need to install another library to remove names (we have only removed HTML)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFpVdl9W9MQ9"
      },
      "outputs": [],
      "source": [
        "!pip install spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kidGUZLR9dCN"
      },
      "source": [
        "Now we can scrape our chat for our names. Don't worry the AI can still understand who is who. ðŸ˜€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NogQn8Ff9-c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Assuming newdf is your DataFrame and 'column_name' is the name of the column you want to process\n",
        "# Replace 'column_name' with the actual name of your column\n",
        "column_name = 'Body'\n",
        "\n",
        "# Function to anonymize text by replacing person names with [REDACTED]\n",
        "def anonymize_text(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            text = text.replace(ent.text, \"[REDACTED]\")\n",
        "    return text\n",
        "\n",
        "# Apply the anonymization function to the specified column\n",
        "bleacheddf[column_name] = bleacheddf[column_name].apply(anonymize_text)\n",
        "\n",
        "# Print the DataFrame to verify changes\n",
        "print(bleacheddf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDFiAQDb-ODd"
      },
      "source": [
        "You can run the export module from above to see the new output - since we are not creating a new DF and just over writting the previous DF we do not need to modify the code. That module can be modified so any DF can be exported\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWeKRKurFqXj"
      },
      "source": [
        "Let's add one more pass to remove unused tokens and articles and line breaks (this makes the prompt cheaper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGzqb7zEFxWa",
        "outputId": "edb2363f-e325-481d-d47f-cbd5dd2e6d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                Body                SFID\n",
            "0  Chat Origin: United States Agent [REDACTED]( 7...  5707S0000012U7jQAE\n",
            "1  [REDACTED]: Denmark Agent [REDACTED]( 28s ) [R...  5707S00000124QQQAY\n",
            "2  [REDACTED]: France Agent [REDACTED] D( 19m 37s...  5707S0000012ES7QAM\n",
            "3  [REDACTED]: United States Agent [REDACTED]( 31...  5707S0000012AKOQA2\n",
            "4  [REDACTED]: United States Agent Mj B( 5m 54s )...  5707S0000012F12QAE\n",
            "5  [REDACTED]: United States Agent [REDACTED]( 48...  5707S0000012fh9QAA\n",
            "6  Chat Origin: United States Agent [REDACTED]( 1...  5707S0000012eZYQAY\n",
            "7  [REDACTED]: United States Agent [REDACTED]( 2m...  5707S00000129qnQAA\n",
            "8  [REDACTED]: United States Agent [REDACTED]( 22...  5707S0000012CctQAE\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Sample DataFrame creation (assuming you already have a DataFrame)\n",
        "# data = {'ID': [1, 2], 'Transcript': ['<p>Chat transcript 1</p>', '<p>Chat transcript 2</p>']}\n",
        "# Function to clean and format a single chat transcript\n",
        "def clean_transcript(Body):\n",
        "    # Remove HTML tags\n",
        "    cleaned_transcript = re.sub(r'<[^>]+>', '', Body)\n",
        "\n",
        "    # Extract and format dialogues\n",
        "    dialogues = re.findall(r'\\(.*?\\)\\s(.*?):\\s(.*?)\\s*(?:<br>|\\Z)', cleaned_transcript, re.DOTALL)\n",
        "\n",
        "    formatted_transcript = []\n",
        "    for agent, text in dialogues:\n",
        "        formatted_transcript.append(f\"{agent}: {text}\")\n",
        "\n",
        "    return \"\\n\".join(formatted_transcript)\n",
        "\n",
        "# Applying the function to the DataFrame\n",
        "def process_transcripts(bleacheddf, Body):\n",
        "    bleacheddf[Body] = bleacheddf[Body].apply(clean_transcript)\n",
        "    return bleacheddf\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the column with transcripts is named 'Transcript'\n",
        "bleacheddf = process_transcripts(bleacheddf, 'Body')\n",
        "print(bleacheddf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-8ZiCzGS3T"
      },
      "source": [
        "### Step 4\n",
        "\n",
        "At this point we are safe to process through and loop our data as the prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGm17_guKE1b"
      },
      "source": [
        "Let's make it easy and set our system prompt first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cG17N5mpKHjq"
      },
      "outputs": [],
      "source": [
        "# System prompt\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert at understanding conversations between travel agents and customers.\n",
        "When given a transcript of a conversation, you will respond with a JSON structure using the following format:\n",
        "{\n",
        "  \"Summary\": <string>,\n",
        "  \"Actions\": <array of strings>,\n",
        "  \"Intents\": <array of strings>,\n",
        "  \"Sentiment\": <string>,\n",
        "  \"Arranger\": <string>,\n",
        "  \"FCR\": <string>,\n",
        "  \"Reason\": <array of strings>,\n",
        "  \"Coaching\": <string>\n",
        "}\n",
        "The fields can be described as follows:\n",
        "Summary: a concise summary of the conversation\n",
        "\n",
        "Actions: concise multi word phrases joined by a - character, like \"search-flight\", \"lookup-itinerary\", \"book-hotel\", \"cancel-flight\". these describe actions that the agent took while in the conversation\n",
        "\n",
        "Intents: concise multi word phrases joined by a - character, like \"quote-flight\", \"cancel-flight\", \"change-hotel\", \"request-invoice\". these describe the intent of the customer answering the question of why did they reach out and contact the travel agent\n",
        "\n",
        "Sentiment: this describes the attitude of the customer by the end of the conversation. possible values include \"positive\" for when they are happy or satisfied, \"negative\" for when they are unhappy or unsatisfied and finally \"neutral\" if there is uncertainty on how they are feeling.\n",
        "\n",
        "Arranger: this describes in a single word \"arranger\" or \"self\" if the person contacting is requesting assistance on behalf of themselves or someone else. If the customer needs help for anyone but themselves it should be labeled as arranger\n",
        "\n",
        "FCR: First chat resolution, in a single word (yes/no) please identify if the caller's issue was resolved on the call or if they will require additional support to resolve their issue\n",
        "\n",
        "Reason: Pick applicable items from the following list, pick only items from this list but pick all applicable items from this list. [Book Flight, Book Hotel, Book Car Rental, Change/Cancel Existing Reservation, Request Travel Policy Information, Expense Reimbursement Question, Invoice or Receipt Request]\n",
        "\n",
        "Coaching: This is the single best coaching you can recommend to the agent based on the conversation. Must be a single sentence.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXwFONYUGbFj",
        "outputId": "78e4d1bd-6c63-4823-f301-a8c1362e9b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 0 rows\n",
            "Processing complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import anthropic\n",
        "import json\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=\"ApiKey\"\n",
        ")\n",
        "\n",
        "# System prompt remains the same\n",
        "\n",
        "def analyze_conversation(transcript):\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "        model=\"claude-3-5-sonnet-20240620\",\n",
        "        max_tokens=1000,\n",
        "        temperature=0,\n",
        "            system=SYSTEM_PROMPT,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": transcript\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Parse the JSON response\n",
        "        json_response = json.loads(response.content[0].text)\n",
        "        return json_response\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from response: {response.content[0].text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Assuming you have a DataFrame named 'df'\n",
        "# Loop through each row in the DataFrame\n",
        "for index, row in bleacheddf.iterrows():\n",
        "    transcript = row['Body']  # Assuming the column is named 'Body'\n",
        "\n",
        "    analysis = analyze_conversation(transcript)\n",
        "\n",
        "    if analysis:\n",
        "        # Update the DataFrame with the analysis results\n",
        "        bleacheddf.at[index, 'Summary'] = analysis.get('Summary', '')\n",
        "        bleacheddf.at[index, 'Actions'] = ', '.join(analysis.get('Actions', []))\n",
        "        bleacheddf.at[index, 'Intents'] = ', '.join(analysis.get('Intents', []))\n",
        "        bleacheddf.at[index, 'Sentiment'] = analysis.get('Sentiment', '')\n",
        "        bleacheddf.at[index, 'Arranger'] = analysis.get('Arranger', '')\n",
        "        bleacheddf.at[index, 'FCR'] = analysis.get('FCR', '')\n",
        "        bleacheddf.at[index, 'Reason'] = ', '.join(analysis.get('Reason', []))\n",
        "        bleacheddf.at[index, 'Coaching'] = analysis.get('Coaching', '')\n",
        "    else:\n",
        "        print(f\"Failed to analyze row {index}\")\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if index % 10 == 0:\n",
        "        print(f\"Processed {index} rows\")\n",
        "\n",
        "# Optionally, you can save the updated DataFrame\n",
        "# df.to_csv('updated_travel_data.csv', index=False)\n",
        "\n",
        "print(\"Processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeuWX_TnLCIS"
      },
      "source": [
        "Now we can print our data to verify before export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RDdF-ilLE0b",
        "outputId": "e63c8f12-a625-4fda-f70f-c527b49c4978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                Body                SFID  \\\n",
            "0  Chat Origin: United States Agent [REDACTED]( 7...  5707S0000012U7jQAE   \n",
            "1  [REDACTED]: Denmark Agent [REDACTED]( 28s ) [R...  5707S00000124QQQAY   \n",
            "2  [REDACTED]: France Agent [REDACTED] D( 19m 37s...  5707S0000012ES7QAM   \n",
            "3  [REDACTED]: United States Agent [REDACTED]( 31...  5707S0000012AKOQA2   \n",
            "4  [REDACTED]: United States Agent Mj B( 5m 54s )...  5707S0000012F12QAE   \n",
            "5  [REDACTED]: United States Agent [REDACTED]( 48...  5707S0000012fh9QAA   \n",
            "6  Chat Origin: United States Agent [REDACTED]( 1...  5707S0000012eZYQAY   \n",
            "7  [REDACTED]: United States Agent [REDACTED]( 2m...  5707S00000129qnQAA   \n",
            "8  [REDACTED]: United States Agent [REDACTED]( 22...  5707S0000012CctQAE   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  A customer arrived in San Francisco and discov...   \n",
            "1  A customer contacted the travel agent seeking ...   \n",
            "2  The customer contacted the agent regarding a c...   \n",
            "3  The customer contacted the agent seeking a rec...   \n",
            "4  The customer wanted to change their flight fro...   \n",
            "5  The customer contacted the agent to change the...   \n",
            "6  The agent greeted the customer and attempted t...   \n",
            "7  The customer needed to adjust their flight to ...   \n",
            "8  The customer requested to change their return ...   \n",
            "\n",
            "                                             Actions  \\\n",
            "0              lookup-itinerary, search-billing-code   \n",
            "1  lookup-itinerary, search-receipts, provide-inv...   \n",
            "2  lookup-itinerary, search-train-options, explai...   \n",
            "3  lookup-itinerary, search-hotel-receipt, send-r...   \n",
            "4  lookup-itinerary, check-flight-change, calcula...   \n",
            "5      lookup-itinerary, provide-contact-information   \n",
            "6                 greet-customer, attempt-engagement   \n",
            "7  lookup-itinerary, attempt-flight-change, provi...   \n",
            "8       lookup-itinerary, change-flight, select-seat   \n",
            "\n",
            "                                            Intents Sentiment  Arranger  FCR  \\\n",
            "0  resolve-payment-issue, request-emergency-contact  negative      self   no   \n",
            "1          request-invoice, locate-missing-receipts   neutral      self   no   \n",
            "2         change-train-ticket, explore-alternatives   neutral  arranger   no   \n",
            "3                                   request-invoice   neutral  arranger   no   \n",
            "4                     change-flight, verify-pricing   neutral      self   no   \n",
            "5                                     change-flight   neutral      self   no   \n",
            "6                                                     neutral      self   no   \n",
            "7                                     change-flight   neutral      self   no   \n",
            "8                                     change-flight  positive      self  yes   \n",
            "\n",
            "                                              Reason  \\\n",
            "0  Book Car Rental, Change/Cancel Existing Reserv...   \n",
            "1                         Invoice or Receipt Request   \n",
            "2                 Change/Cancel Existing Reservation   \n",
            "3                         Invoice or Receipt Request   \n",
            "4                 Change/Cancel Existing Reservation   \n",
            "5                 Change/Cancel Existing Reservation   \n",
            "6                                                      \n",
            "7                 Change/Cancel Existing Reservation   \n",
            "8                 Change/Cancel Existing Reservation   \n",
            "\n",
            "                                            Coaching  \n",
            "0  Respond more quickly to the customer's urgent ...  \n",
            "1  Follow up with the customer via email with any...  \n",
            "2  Proactively offer to follow up with the custom...  \n",
            "3  Always confirm the exact nature of the receipt...  \n",
            "4  Provide more detailed explanations about fare ...  \n",
            "5  The agent could have offered to transfer the c...  \n",
            "6  Consider implementing a more proactive approac...  \n",
            "7  The agent should have recognized the complexit...  \n",
            "8  Double-check flight details with the customer ...  \n"
          ]
        }
      ],
      "source": [
        "print(bleacheddf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzWzG2YdLOH6"
      },
      "source": [
        "Obviously this is a bit hard to read so lets convert this data into an excel friendly version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZBNSR0-Hz2S"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ7UjPdWLnha"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from google.colab import files\n",
        "\n",
        "def export_df_to_excel(bleacheddf, filename='dataframe_export.xlsx'):\n",
        "    \"\"\"\n",
        "    Export a pandas DataFrame to an Excel file and download it in Google Colab.\n",
        "\n",
        "    Parameters:\n",
        "    bleacheddf (pandas.DataFrame): The DataFrame to export\n",
        "    filename (str): The name of the file to be downloaded (default is 'dataframe_export.xlsx')\n",
        "    \"\"\"\n",
        "    # Create a BytesIO object\n",
        "    output = BytesIO()\n",
        "\n",
        "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
        "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
        "        # Write the DataFrame to the Excel file\n",
        "        bleacheddf.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "\n",
        "        # Get the xlsxwriter workbook and worksheet objects\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "        # Set the column widths (adjust as needed)\n",
        "        for i, col in enumerate(bleacheddf.columns):\n",
        "            column_len = max(bleacheddf[col].astype(str).str.len().max(), len(col) + 2)\n",
        "            worksheet.set_column(i, i, column_len)\n",
        "\n",
        "    # Reset the buffer position to the beginning\n",
        "    output.seek(0)\n",
        "\n",
        "    # Write the BytesIO content to the file\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(output.getvalue())\n",
        "\n",
        "    # Use google.colab.files.download to trigger the file download\n",
        "    files.download(filename)\n",
        "\n",
        "# Example usage\n",
        "# Create a sample DataFrame\n",
        "data = bleacheddf\n",
        "bleacheddf = pd.DataFrame(data)\n",
        "\n",
        "# Export the DataFrame to an Excel file\n",
        "export_df_to_excel(bleacheddf, 'final.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7cWfXX3Ls_K"
      },
      "source": [
        "And there we go! We have a fully automated pipline - if all tested well, you can go download a new set of chats, make sure there are only two columns (or three can't remember haha) and that the transcript column is named Body.\n",
        "\n",
        "Note: all these paramaters can be changed in the code, thats just how the code is functioning now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FND-kDTrL6oK"
      },
      "source": [
        "This is the EXACT workbench query you can use to pull your data - you can update the limits, add filters and change dates etc\n",
        "\n",
        "```\n",
        "SELECT Body,Id FROM LiveChatTranscript WHERE Language__c = 'English' AND OperatorMessageCount > 1 AND CreatedDate > 2024-01-08T00:00:00Z LIMIT 5\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
